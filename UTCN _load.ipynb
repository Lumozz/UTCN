{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2 as cv\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import weight_norm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1  超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "EPOCHS = 5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 载入数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class utcntrain(Dataset):#数据集\n",
    "\n",
    "    def __init__(self):\n",
    "    \n",
    "        '''读训练集图片'''\n",
    "   \n",
    "        with open ('data/train_train_image.txt','r') as tf:\n",
    "            train_image_file = tf.read().splitlines()    #这种读取方式和readlines()一样，只是去掉了每一行后面的换行符\\n\n",
    "\n",
    "        with open ('data/train_train_label.txt','r') as tf:\n",
    "            train_label_file = tf.read().splitlines()    #这种读取方式和readlines()一样，只是去掉了每一行后面的换行符\\n\n",
    "\n",
    "\n",
    "        self.train_image=train_image_file\n",
    "        self.train_label=train_label_file\n",
    "\n",
    "        \n",
    "    def __len__(self):# 返回df的长度\n",
    "      \n",
    "        return len(self.train_image)\n",
    "    \n",
    "    def __getitem__(self, idx):#idx是索引，根据这个索引返回一个样本\n",
    "        if idx >= 19097:\n",
    "            trainimage1 = cv.imread(self.train_image[idx]+'/10.jpg')\n",
    "            trainimage2 = cv.imread(self.train_image[idx]+'/11.jpg')\n",
    "            trainimage3 = cv.imread(self.train_image[idx]+'/12.jpg')\n",
    "            trainimage4 = cv.imread(self.train_image[idx]+'/13.jpg')\n",
    "            trainimage1 = np.transpose(trainimage1, (2,0,1))\n",
    "            trainimage2 = np.transpose(trainimage2, (2,0,1))\n",
    "            trainimage3 = np.transpose(trainimage3, (2,0,1))\n",
    "            trainimage4 = np.transpose(trainimage4, (2,0,1))\n",
    "                                      \n",
    "        else:\n",
    "            trainimage1 = cv.imread(self.train_image[idx]+'/17.jpg')\n",
    "            trainimage2 = cv.imread(self.train_image[idx]+'/18.jpg')\n",
    "            trainimage3 = cv.imread(self.train_image[idx]+'/19.jpg')\n",
    "            trainimage4 = cv.imread(self.train_image[idx]+'/20.jpg')\n",
    "            trainimage1 = np.transpose(trainimage1, (2,0,1))\n",
    "            trainimage2 = np.transpose(trainimage2, (2,0,1))\n",
    "            trainimage3 = np.transpose(trainimage3, (2,0,1))\n",
    "            trainimage4 = np.transpose(trainimage4, (2,0,1))\n",
    "       \n",
    "        trainimage = np.stack((trainimage1,trainimage2,trainimage3,trainimage4))\n",
    "        trainimage = trainimage/225\n",
    "        \n",
    "        labimage = cv.imread(self.train_label[idx]+'.jpg',0)#读灰度图\n",
    "        ret,labimage = cv.threshold(labimage, 0, 1, cv.THRESH_BINARY | cv.THRESH_TRIANGLE)#二值化\n",
    "        labimage = np.array([labimage])\n",
    "        #labimage = np.transpose(labimage, (2,0,1))\n",
    "        return torch.from_numpy(trainimage).float(),torch.from_numpy(labimage).float() #读一张图为numpy,返回一个列表里两个tensor。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class utcnval(Dataset):#数据集\n",
    "\n",
    "    def __init__(self):\n",
    "    \n",
    "        '''读训练集图片'''\n",
    "   \n",
    "        with open ('data/train_val_image.txt','r') as tf:\n",
    "            train_image_file = tf.read().splitlines()    #这种读取方式和readlines()一样，只是去掉了每一行后面的换行符\\n\n",
    "\n",
    "        with open ('data/train_val_label.txt','r') as tf:\n",
    "            train_label_file = tf.read().splitlines()    #这种读取方式和readlines()一样，只是去掉了每一行后面的换行符\\n\n",
    "\n",
    "\n",
    "        self.train_image=train_image_file\n",
    "        self.train_label=train_label_file\n",
    "\n",
    "        \n",
    "    def __len__(self):# 返回df的长度\n",
    "      \n",
    "        return len(self.train_image)\n",
    "    \n",
    "    def __getitem__(self, idx):#idx是索引，根据这个索引返回一个样本\n",
    "        if idx >= 19097:\n",
    "            trainimage1 = cv.imread(self.train_image[idx]+'/10.jpg')\n",
    "            trainimage2 = cv.imread(self.train_image[idx]+'/11.jpg')\n",
    "            trainimage3 = cv.imread(self.train_image[idx]+'/12.jpg')\n",
    "            trainimage4 = cv.imread(self.train_image[idx]+'/13.jpg')\n",
    "            trainimage1 = np.transpose(trainimage1, (2,0,1))\n",
    "            trainimage2 = np.transpose(trainimage2, (2,0,1))\n",
    "            trainimage3 = np.transpose(trainimage3, (2,0,1))\n",
    "            trainimage4 = np.transpose(trainimage4, (2,0,1))\n",
    "                                      \n",
    "        else:\n",
    "            trainimage1 = cv.imread(self.train_image[idx]+'/17.jpg')\n",
    "            trainimage2 = cv.imread(self.train_image[idx]+'/18.jpg')\n",
    "            trainimage3 = cv.imread(self.train_image[idx]+'/19.jpg')\n",
    "            trainimage4 = cv.imread(self.train_image[idx]+'/20.jpg')\n",
    "            trainimage1 = np.transpose(trainimage1, (2,0,1))\n",
    "            trainimage2 = np.transpose(trainimage2, (2,0,1))\n",
    "            trainimage3 = np.transpose(trainimage3, (2,0,1))\n",
    "            trainimage4 = np.transpose(trainimage4, (2,0,1))\n",
    "       \n",
    "        trainimage = np.stack((trainimage1,trainimage2,trainimage3,trainimage4))\n",
    "        trainimage = trainimage/225\n",
    "        \n",
    "        labimage = cv.imread(self.train_label[idx]+'.jpg',0)#读灰度图\n",
    "        ret,labimage = cv.threshold(labimage, 0, 1, cv.THRESH_BINARY | cv.THRESH_TRIANGLE)#二值化\n",
    "        labimage = np.array([labimage])\n",
    "        #labimage = np.transpose(labimage, (2,0,1))\n",
    "        return torch.from_numpy(trainimage).float(),torch.from_numpy(labimage).float() #读一张图为numpy,返回一个列表里两个tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "载入train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = utcntrain()#参数为路径\n",
    "train_loader = torch.utils.data.DataLoader(traindata, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "valdata = utcnval()#参数为路径\n",
    "val_loader = torch.utils.data.DataLoader(valdata, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------Test code start------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    data, target\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------Test code end------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 TCN部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 裁剪多余的Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        \n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size[0]\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x=x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 TCN主体（采用3d卷积+残差）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = weight_norm(nn.Conv3d(n_inputs, n_outputs, \n",
    "                                            kernel_size=(2,3,3), stride=1, padding=(0,1,1), dilation=(1,1,1), bias=True))\n",
    "        #self.chomp1 = Chomp1d([1,1,1])  \n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = weight_norm(nn.Conv3d(n_inputs, n_outputs, \n",
    "                                            kernel_size=(2,3,3), stride=1, padding=(0,1,1), dilation=(2,1,1), bias=True))\n",
    "        #self.chomp2 = Chomp1d([2,1,1])  \n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.net = nn.Sequential(self.conv1, self.relu1,\n",
    "                                self.conv2,  self.relu2)\n",
    "        \n",
    "        \n",
    "        #self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1,\n",
    "         #                       self.conv2, self.chomp2, self.relu2)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = self.net(x)\n",
    "       \n",
    "        res = x \n",
    "       \n",
    "        \n",
    "        return self.relu(out + res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 UNet部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels // 2, in_channels // 2, kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = torch.tensor([x2.size()[2] - x1.size()[2]])\n",
    "        diffX = torch.tensor([x2.size()[3] - x1.size()[3]])\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3整合（UNet下采样+TCN+UNet上采样）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UTCN(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(UTCN, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 512)\n",
    "        self.up1 = Up(1024, 256, bilinear)\n",
    "        self.up2 = Up(512, 128, bilinear)\n",
    "        self.up3 = Up(256, 64, bilinear)\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    " \n",
    "        self.tcn = TemporalBlock(512,512) #TCN\n",
    "    \n",
    "    def forward(self, x):\n",
    "        xTCN = []\n",
    "        for i in range(0,4):\n",
    "            if i==3:\n",
    "                x1 = self.inc(x[i])\n",
    "                x2 = self.down1(x1)\n",
    "                x3 = self.down2(x2)\n",
    "                x4 = self.down3(x3)\n",
    "                x5 = self.down4(x4)\n",
    "                xTCN.append(x5)\n",
    "            else:\n",
    "                xd = self.inc(x[i])\n",
    "                xd = self.down1(xd)\n",
    "                xd = self.down2(xd)\n",
    "                xd = self.down3(xd)\n",
    "                xd = self.down4(xd)\n",
    "                xTCN.append(xd)\n",
    "        xTCN=torch.stack((xTCN[0],xTCN[1],xTCN[2],xTCN[3])) #xTCN.shape()=(D,batch_size,C,H,W)\n",
    "        \n",
    "        xTCN=xTCN.permute(1,2,0,3,4)  #输入TCN时，变成（batch_size,C,D,H,W)\n",
    "        \n",
    "        xTCN_output = self.tcn(xTCN)\n",
    "        \n",
    "        xTCN_output = xTCN_output.permute(2,0,1,3,4) #改回(D,batch_size,C,H,W)\n",
    "        \n",
    "        xTCN_output = xTCN_output[3]    #只有最后一个有用\n",
    "        \n",
    "        x = self.up1(xTCN_output, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        logits = torch.sigmoid(logits)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 FocalLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2, weight=None, ignore_index=255):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight\n",
    "        self.ignore_index = ignore_index\n",
    "        self.bce_fn = nn.BCEWithLogitsLoss(weight=self.weight)\n",
    "\n",
    "    def forward(self, preds, labels):\n",
    "        if self.ignore_index is not None:\n",
    "            mask = labels != self.ignore_index\n",
    "            labels = labels[mask]\n",
    "            preds = preds[mask]\n",
    "\n",
    "        logpt = -self.bce_fn(preds, labels)\n",
    "        pt = torch.exp(logpt)\n",
    "        loss = -((1 - pt) ** self.gamma) * self.alpha * logpt\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class_weight = [0.02, 1.02]\n",
    "class_weight = torch.Tensor(class_weight)\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weight).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 DiceLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    " \n",
    "    def forward(self, logits, targets):\n",
    "        num = targets.size(0)\n",
    "        smooth = 0\n",
    "        \n",
    "        m1 = logits.view(num, -1)\n",
    "        m2 = targets.view(num, -1)\n",
    "\n",
    "        intersection = (m1 * m2)\n",
    " \n",
    "        score = 2. * (intersection.sum(1) + smooth) / (m1.sum(1) + m2.sum(1) + smooth)\n",
    "        score = 1 - score.sum() / num\n",
    "        return score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UTCN(3,1).to(DEVICE)\n",
    "model = torch.nn.DataParallel(model)\n",
    "#optimizer = optim.Adam(model.parameters(),lr=0.005, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = FocalLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('utcn5.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 定义训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, criterion):\n",
    "    model.train()\n",
    "    since = time.time()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()        \n",
    "        data = data.permute(1,0,2,3,4)#(D,B,C,W,H)\n",
    "        output = model(data)\n",
    "        lossfn = criterion \n",
    "        loss = lossfn(output,target)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        if(batch_idx+1)%10 == 0: \n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * BATCH_SIZE, len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            with open('lossdata.txt','a') as f:    #设置文件对象\n",
    "                f.write('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * BATCH_SIZE, len(train_loader.dataset),\n",
    "                100. * batch_idx/ len(train_loader), loss.item())+'\\n')\n",
    "                \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Train Epoch: {} complete in {:.0f}m {:.0f}s'.format(epoch,\n",
    "        time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 验证函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(model, device, val_loader, criterion):\n",
    "    model.eval()\n",
    "    since = time.time()\n",
    "    loss_sum=[]\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(val_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "           \n",
    "            \n",
    "            data = data.permute(1,0,2,3,4)#(D,B,C,W,H)\n",
    "            output = model(data)\n",
    "            lossfn = criterion \n",
    "            loss = lossfn(output,target)\n",
    "            \n",
    "            loss_sum.append(loss.item())\n",
    "\n",
    "            if(batch_idx+1)%10== 0: \n",
    "                print('Trainval Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * BATCH_SIZE, len(val_loader.dataset),\n",
    "                    100. * batch_idx / len(val_loader), loss.item()))\n",
    "                \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Train Epoch: {} complete in {:.0f}m {:.0f}s,lossave = {}'.format(epoch,\n",
    "        time_elapsed // 60, time_elapsed % 60,sum(loss_sum)/len(loss_sum)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "val(model, DEVICE, val_loader, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [180/34192 (1%)]\tLoss: 0.043936\n",
      "Train Epoch: 1 [380/34192 (1%)]\tLoss: 0.043919\n",
      "Train Epoch: 1 [580/34192 (2%)]\tLoss: 0.043656\n",
      "Train Epoch: 1 [780/34192 (2%)]\tLoss: 0.043904\n",
      "Train Epoch: 1 [980/34192 (3%)]\tLoss: 0.043887\n",
      "Train Epoch: 1 [1180/34192 (3%)]\tLoss: 0.044208\n",
      "Train Epoch: 1 [1380/34192 (4%)]\tLoss: 0.044052\n",
      "Train Epoch: 1 [1580/34192 (5%)]\tLoss: 0.044033\n",
      "Train Epoch: 1 [1780/34192 (5%)]\tLoss: 0.043667\n",
      "Train Epoch: 1 [1980/34192 (6%)]\tLoss: 0.044026\n",
      "Train Epoch: 1 [2180/34192 (6%)]\tLoss: 0.043987\n",
      "Train Epoch: 1 [2380/34192 (7%)]\tLoss: 0.043752\n",
      "Train Epoch: 1 [2580/34192 (8%)]\tLoss: 0.043955\n",
      "Train Epoch: 1 [2780/34192 (8%)]\tLoss: 0.044045\n",
      "Train Epoch: 1 [2980/34192 (9%)]\tLoss: 0.043804\n",
      "Train Epoch: 1 [3180/34192 (9%)]\tLoss: 0.044107\n",
      "Train Epoch: 1 [3380/34192 (10%)]\tLoss: 0.043868\n",
      "Train Epoch: 1 [3580/34192 (10%)]\tLoss: 0.043667\n",
      "Train Epoch: 1 [3780/34192 (11%)]\tLoss: 0.044171\n",
      "Train Epoch: 1 [3980/34192 (12%)]\tLoss: 0.044026\n",
      "Train Epoch: 1 [4180/34192 (12%)]\tLoss: 0.043976\n",
      "Train Epoch: 1 [4380/34192 (13%)]\tLoss: 0.044132\n",
      "Train Epoch: 1 [4580/34192 (13%)]\tLoss: 0.043958\n",
      "Train Epoch: 1 [4780/34192 (14%)]\tLoss: 0.044023\n",
      "Train Epoch: 1 [4980/34192 (15%)]\tLoss: 0.043934\n",
      "Train Epoch: 1 [5180/34192 (15%)]\tLoss: 0.043918\n",
      "Train Epoch: 1 [5380/34192 (16%)]\tLoss: 0.043951\n",
      "Train Epoch: 1 [5580/34192 (16%)]\tLoss: 0.044013\n",
      "Train Epoch: 1 [5780/34192 (17%)]\tLoss: 0.043925\n",
      "Train Epoch: 1 [5980/34192 (17%)]\tLoss: 0.043802\n",
      "Train Epoch: 1 [6180/34192 (18%)]\tLoss: 0.043779\n",
      "Train Epoch: 1 [6380/34192 (19%)]\tLoss: 0.043955\n",
      "Train Epoch: 1 [6580/34192 (19%)]\tLoss: 0.043928\n",
      "Train Epoch: 1 [6780/34192 (20%)]\tLoss: 0.043892\n",
      "Train Epoch: 1 [6980/34192 (20%)]\tLoss: 0.044025\n",
      "Train Epoch: 1 [7180/34192 (21%)]\tLoss: 0.044062\n",
      "Train Epoch: 1 [7380/34192 (22%)]\tLoss: 0.043948\n",
      "Train Epoch: 1 [7580/34192 (22%)]\tLoss: 0.043894\n",
      "Train Epoch: 1 [7780/34192 (23%)]\tLoss: 0.043847\n",
      "Train Epoch: 1 [7980/34192 (23%)]\tLoss: 0.044025\n",
      "Train Epoch: 1 [8180/34192 (24%)]\tLoss: 0.043859\n",
      "Train Epoch: 1 [8380/34192 (25%)]\tLoss: 0.044004\n",
      "Train Epoch: 1 [8580/34192 (25%)]\tLoss: 0.043935\n",
      "Train Epoch: 1 [8780/34192 (26%)]\tLoss: 0.044046\n",
      "Train Epoch: 1 [8980/34192 (26%)]\tLoss: 0.043998\n",
      "Train Epoch: 1 [9180/34192 (27%)]\tLoss: 0.043779\n",
      "Train Epoch: 1 [9380/34192 (27%)]\tLoss: 0.043842\n",
      "Train Epoch: 1 [9580/34192 (28%)]\tLoss: 0.043893\n",
      "Train Epoch: 1 [9780/34192 (29%)]\tLoss: 0.043884\n",
      "Train Epoch: 1 [9980/34192 (29%)]\tLoss: 0.043942\n",
      "Train Epoch: 1 [10180/34192 (30%)]\tLoss: 0.043882\n",
      "Train Epoch: 1 [10380/34192 (30%)]\tLoss: 0.043779\n",
      "Train Epoch: 1 [10580/34192 (31%)]\tLoss: 0.043936\n",
      "Train Epoch: 1 [10780/34192 (32%)]\tLoss: 0.044009\n",
      "Train Epoch: 1 [10980/34192 (32%)]\tLoss: 0.043786\n",
      "Train Epoch: 1 [11180/34192 (33%)]\tLoss: 0.044055\n",
      "Train Epoch: 1 [11380/34192 (33%)]\tLoss: 0.043888\n",
      "Train Epoch: 1 [11580/34192 (34%)]\tLoss: 0.044024\n",
      "Train Epoch: 1 [11780/34192 (34%)]\tLoss: 0.043814\n",
      "Train Epoch: 1 [11980/34192 (35%)]\tLoss: 0.043781\n",
      "Train Epoch: 1 [12180/34192 (36%)]\tLoss: 0.044064\n",
      "Train Epoch: 1 [12380/34192 (36%)]\tLoss: 0.043967\n",
      "Train Epoch: 1 [12580/34192 (37%)]\tLoss: 0.044062\n",
      "Train Epoch: 1 [12780/34192 (37%)]\tLoss: 0.044140\n",
      "Train Epoch: 1 [12980/34192 (38%)]\tLoss: 0.044013\n",
      "Train Epoch: 1 [13180/34192 (39%)]\tLoss: 0.043971\n",
      "Train Epoch: 1 [13380/34192 (39%)]\tLoss: 0.043786\n",
      "Train Epoch: 1 [13580/34192 (40%)]\tLoss: 0.044026\n",
      "Train Epoch: 1 [13780/34192 (40%)]\tLoss: 0.044167\n",
      "Train Epoch: 1 [13980/34192 (41%)]\tLoss: 0.044022\n",
      "Train Epoch: 1 [14180/34192 (41%)]\tLoss: 0.044047\n",
      "Train Epoch: 1 [14380/34192 (42%)]\tLoss: 0.043898\n",
      "Train Epoch: 1 [14580/34192 (43%)]\tLoss: 0.044282\n",
      "Train Epoch: 1 [14780/34192 (43%)]\tLoss: 0.044059\n",
      "Train Epoch: 1 [14980/34192 (44%)]\tLoss: 0.043865\n",
      "Train Epoch: 1 [15180/34192 (44%)]\tLoss: 0.044076\n",
      "Train Epoch: 1 [15380/34192 (45%)]\tLoss: 0.044025\n",
      "Train Epoch: 1 [15580/34192 (46%)]\tLoss: 0.044019\n",
      "Train Epoch: 1 [15780/34192 (46%)]\tLoss: 0.043900\n",
      "Train Epoch: 1 [15980/34192 (47%)]\tLoss: 0.043719\n",
      "Train Epoch: 1 [16180/34192 (47%)]\tLoss: 0.043951\n",
      "Train Epoch: 1 [16380/34192 (48%)]\tLoss: 0.043888\n",
      "Train Epoch: 1 [16580/34192 (48%)]\tLoss: 0.043970\n",
      "Train Epoch: 1 [16780/34192 (49%)]\tLoss: 0.044107\n",
      "Train Epoch: 1 [16980/34192 (50%)]\tLoss: 0.043868\n",
      "Train Epoch: 1 [17180/34192 (50%)]\tLoss: 0.044038\n",
      "Train Epoch: 1 [17380/34192 (51%)]\tLoss: 0.043803\n",
      "Train Epoch: 1 [17580/34192 (51%)]\tLoss: 0.043906\n",
      "Train Epoch: 1 [17780/34192 (52%)]\tLoss: 0.043866\n",
      "Train Epoch: 1 [17980/34192 (53%)]\tLoss: 0.044050\n",
      "Train Epoch: 1 [18180/34192 (53%)]\tLoss: 0.043773\n",
      "Train Epoch: 1 [18380/34192 (54%)]\tLoss: 0.043818\n",
      "Train Epoch: 1 [18580/34192 (54%)]\tLoss: 0.043892\n",
      "Train Epoch: 1 [18780/34192 (55%)]\tLoss: 0.043858\n",
      "Train Epoch: 1 [18980/34192 (55%)]\tLoss: 0.043748\n",
      "Train Epoch: 1 [19180/34192 (56%)]\tLoss: 0.043940\n",
      "Train Epoch: 1 [19380/34192 (57%)]\tLoss: 0.043845\n",
      "Train Epoch: 1 [19580/34192 (57%)]\tLoss: 0.044117\n",
      "Train Epoch: 1 [19780/34192 (58%)]\tLoss: 0.044139\n",
      "Train Epoch: 1 [19980/34192 (58%)]\tLoss: 0.043950\n",
      "Train Epoch: 1 [20180/34192 (59%)]\tLoss: 0.043764\n",
      "Train Epoch: 1 [20380/34192 (60%)]\tLoss: 0.044064\n",
      "Train Epoch: 1 [20580/34192 (60%)]\tLoss: 0.043804\n",
      "Train Epoch: 1 [20780/34192 (61%)]\tLoss: 0.044072\n",
      "Train Epoch: 1 [20980/34192 (61%)]\tLoss: 0.044048\n",
      "Train Epoch: 1 [21180/34192 (62%)]\tLoss: 0.043924\n",
      "Train Epoch: 1 [21380/34192 (63%)]\tLoss: 0.044067\n",
      "Train Epoch: 1 [21580/34192 (63%)]\tLoss: 0.044113\n",
      "Train Epoch: 1 [21780/34192 (64%)]\tLoss: 0.043854\n",
      "Train Epoch: 1 [21980/34192 (64%)]\tLoss: 0.043967\n",
      "Train Epoch: 1 [22180/34192 (65%)]\tLoss: 0.043837\n",
      "Train Epoch: 1 [22380/34192 (65%)]\tLoss: 0.044163\n",
      "Train Epoch: 1 [22580/34192 (66%)]\tLoss: 0.043831\n",
      "Train Epoch: 1 [22780/34192 (67%)]\tLoss: 0.044016\n",
      "Train Epoch: 1 [22980/34192 (67%)]\tLoss: 0.043901\n",
      "Train Epoch: 1 [23180/34192 (68%)]\tLoss: 0.043913\n",
      "Train Epoch: 1 [23380/34192 (68%)]\tLoss: 0.043899\n",
      "Train Epoch: 1 [23580/34192 (69%)]\tLoss: 0.044142\n",
      "Train Epoch: 1 [23780/34192 (70%)]\tLoss: 0.044019\n",
      "Train Epoch: 1 [23980/34192 (70%)]\tLoss: 0.043933\n",
      "Train Epoch: 1 [24180/34192 (71%)]\tLoss: 0.043992\n",
      "Train Epoch: 1 [24380/34192 (71%)]\tLoss: 0.044045\n",
      "Train Epoch: 1 [24580/34192 (72%)]\tLoss: 0.043966\n",
      "Train Epoch: 1 [24780/34192 (72%)]\tLoss: 0.044078\n",
      "Train Epoch: 1 [24980/34192 (73%)]\tLoss: 0.044045\n",
      "Train Epoch: 1 [25180/34192 (74%)]\tLoss: 0.044124\n",
      "Train Epoch: 1 [25380/34192 (74%)]\tLoss: 0.043712\n",
      "Train Epoch: 1 [25580/34192 (75%)]\tLoss: 0.044092\n",
      "Train Epoch: 1 [25780/34192 (75%)]\tLoss: 0.044227\n",
      "Train Epoch: 1 [25980/34192 (76%)]\tLoss: 0.044023\n",
      "Train Epoch: 1 [26180/34192 (77%)]\tLoss: 0.044164\n",
      "Train Epoch: 1 [26380/34192 (77%)]\tLoss: 0.043737\n",
      "Train Epoch: 1 [26580/34192 (78%)]\tLoss: 0.043807\n",
      "Train Epoch: 1 [26780/34192 (78%)]\tLoss: 0.043807\n",
      "Train Epoch: 1 [26980/34192 (79%)]\tLoss: 0.044214\n",
      "Train Epoch: 1 [27180/34192 (79%)]\tLoss: 0.044034\n",
      "Train Epoch: 1 [27380/34192 (80%)]\tLoss: 0.043831\n",
      "Train Epoch: 1 [27580/34192 (81%)]\tLoss: 0.043999\n",
      "Train Epoch: 1 [27780/34192 (81%)]\tLoss: 0.043695\n",
      "Train Epoch: 1 [27980/34192 (82%)]\tLoss: 0.043999\n",
      "Train Epoch: 1 [28180/34192 (82%)]\tLoss: 0.043919\n",
      "Train Epoch: 1 [28380/34192 (83%)]\tLoss: 0.043775\n",
      "Train Epoch: 1 [28580/34192 (84%)]\tLoss: 0.043984\n",
      "Train Epoch: 1 [28780/34192 (84%)]\tLoss: 0.044023\n",
      "Train Epoch: 1 [28980/34192 (85%)]\tLoss: 0.043821\n",
      "Train Epoch: 1 [29180/34192 (85%)]\tLoss: 0.043766\n",
      "Train Epoch: 1 [29380/34192 (86%)]\tLoss: 0.043963\n",
      "Train Epoch: 1 [29580/34192 (86%)]\tLoss: 0.044049\n",
      "Train Epoch: 1 [29780/34192 (87%)]\tLoss: 0.043867\n",
      "Train Epoch: 1 [29980/34192 (88%)]\tLoss: 0.044172\n",
      "Train Epoch: 1 [30180/34192 (88%)]\tLoss: 0.043808\n",
      "Train Epoch: 1 [30380/34192 (89%)]\tLoss: 0.043789\n",
      "Train Epoch: 1 [30580/34192 (89%)]\tLoss: 0.043757\n",
      "Train Epoch: 1 [30780/34192 (90%)]\tLoss: 0.044054\n",
      "Train Epoch: 1 [30980/34192 (91%)]\tLoss: 0.043706\n",
      "Train Epoch: 1 [31180/34192 (91%)]\tLoss: 0.044155\n",
      "Train Epoch: 1 [31380/34192 (92%)]\tLoss: 0.043933\n",
      "Train Epoch: 1 [31580/34192 (92%)]\tLoss: 0.044021\n",
      "Train Epoch: 1 [31780/34192 (93%)]\tLoss: 0.044019\n",
      "Train Epoch: 1 [31980/34192 (94%)]\tLoss: 0.044102\n",
      "Train Epoch: 1 [32180/34192 (94%)]\tLoss: 0.044093\n",
      "Train Epoch: 1 [32380/34192 (95%)]\tLoss: 0.043994\n",
      "Train Epoch: 1 [32580/34192 (95%)]\tLoss: 0.043815\n",
      "Train Epoch: 1 [32780/34192 (96%)]\tLoss: 0.043811\n",
      "Train Epoch: 1 [32980/34192 (96%)]\tLoss: 0.043780\n",
      "Train Epoch: 1 [33180/34192 (97%)]\tLoss: 0.044077\n",
      "Train Epoch: 1 [33380/34192 (98%)]\tLoss: 0.043902\n",
      "Train Epoch: 1 [33580/34192 (98%)]\tLoss: 0.043961\n",
      "Train Epoch: 1 [33780/34192 (99%)]\tLoss: 0.043961\n",
      "Train Epoch: 1 [33980/34192 (99%)]\tLoss: 0.044010\n",
      "Train Epoch: 1 [34180/34192 (100%)]\tLoss: 0.043918\n",
      "Train Epoch: 1 complete in 14m 9s\n",
      "Trainval Epoch: 1 [180/4000 (4%)]\tLoss: 0.044097\n",
      "Trainval Epoch: 1 [380/4000 (10%)]\tLoss: 0.044055\n",
      "Trainval Epoch: 1 [580/4000 (14%)]\tLoss: 0.043812\n",
      "Trainval Epoch: 1 [780/4000 (20%)]\tLoss: 0.043915\n",
      "Trainval Epoch: 1 [980/4000 (24%)]\tLoss: 0.044178\n",
      "Trainval Epoch: 1 [1180/4000 (30%)]\tLoss: 0.043996\n",
      "Trainval Epoch: 1 [1380/4000 (34%)]\tLoss: 0.044027\n",
      "Trainval Epoch: 1 [1580/4000 (40%)]\tLoss: 0.043998\n",
      "Trainval Epoch: 1 [1780/4000 (44%)]\tLoss: 0.043885\n",
      "Trainval Epoch: 1 [1980/4000 (50%)]\tLoss: 0.044164\n",
      "Trainval Epoch: 1 [2180/4000 (54%)]\tLoss: 0.043851\n",
      "Trainval Epoch: 1 [2380/4000 (60%)]\tLoss: 0.044047\n",
      "Trainval Epoch: 1 [2580/4000 (64%)]\tLoss: 0.044006\n",
      "Trainval Epoch: 1 [2780/4000 (70%)]\tLoss: 0.043816\n",
      "Trainval Epoch: 1 [2980/4000 (74%)]\tLoss: 0.043996\n",
      "Trainval Epoch: 1 [3180/4000 (80%)]\tLoss: 0.043946\n",
      "Trainval Epoch: 1 [3380/4000 (84%)]\tLoss: 0.043923\n",
      "Trainval Epoch: 1 [3580/4000 (90%)]\tLoss: 0.043893\n",
      "Trainval Epoch: 1 [3780/4000 (94%)]\tLoss: 0.043958\n",
      "Trainval Epoch: 1 [3980/4000 (100%)]\tLoss: 0.044023\n",
      "Train Epoch: 1 complete in 0m 38s,lossave = 0.04394757764413953\n",
      "Train Epoch: 2 [180/34192 (1%)]\tLoss: 0.043927\n",
      "Train Epoch: 2 [380/34192 (1%)]\tLoss: 0.043912\n",
      "Train Epoch: 2 [580/34192 (2%)]\tLoss: 0.043649\n",
      "Train Epoch: 2 [780/34192 (2%)]\tLoss: 0.043896\n",
      "Train Epoch: 2 [980/34192 (3%)]\tLoss: 0.043878\n",
      "Train Epoch: 2 [1180/34192 (3%)]\tLoss: 0.044200\n",
      "Train Epoch: 2 [1380/34192 (4%)]\tLoss: 0.044044\n",
      "Train Epoch: 2 [1580/34192 (5%)]\tLoss: 0.044024\n",
      "Train Epoch: 2 [1780/34192 (5%)]\tLoss: 0.043660\n",
      "Train Epoch: 2 [1980/34192 (6%)]\tLoss: 0.044019\n",
      "Train Epoch: 2 [2180/34192 (6%)]\tLoss: 0.043979\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, DEVICE, train_loader, optimizer, epoch,criterion)\n",
    "    val(model, DEVICE, val_loader, criterion)\n",
    "    \n",
    "torch.save(model.state_dict(),'utcn5.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'utcn5.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 调试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('utcn3.pt',map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgtest1 = cv.imread('1.jpg')\n",
    "imgtest2 = cv.imread('2.jpg')\n",
    "imgtest3 = cv.imread('3.jpg')\n",
    "imgtest4 = cv.imread('4.jpg')\n",
    "imgtest1 = np.transpose(imgtest1, (2,0,1))\n",
    "imgtest2 = np.transpose(imgtest2, (2,0,1))\n",
    "imgtest3 = np.transpose(imgtest3, (2,0,1))\n",
    "imgtest4 = np.transpose(imgtest4, (2,0,1))\n",
    "\n",
    "imgtest = np.stack((imgtest1,imgtest2,imgtest3,imgtest4))\n",
    "imgtest = imgtest[np.newaxis,:,:,:,:]\n",
    "#imgtest = torch.from_numpy(imgtest).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4, 3, 128, 256)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1, 3, 128, 256)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgtest = imgtest.transpose(1,0,2,3,4)\n",
    "imgtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgtest = torch.from_numpy(imgtest).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(imgtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 256])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "output1 = output[0][0]\n",
    "#output1=target[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 256)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero = torch.zeros_like(output1)\n",
    "ones = torch.ones_like(output1)*255\n",
    "door = 0.004\n",
    "output2 = torch.where(output1 > door, ones,output1)\n",
    "output2 = torch.where(output2 <= door, zero,output2)\n",
    "output2 = output2.detach().numpy()\n",
    "output2=np.array(output2,dtype='uint8') \n",
    "output2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.imwrite('output1.jpg',output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=data[0][3]\n",
    "a=np.array(a,dtype='uint8') \n",
    "a = a.transpose(1,2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.imwrite('outputt.jpg',a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[0.8267, 0.9467, 0.9422,  ..., 0.2622, 0.2711, 0.2622],\n",
       "           [0.8222, 0.9467, 0.9422,  ..., 0.2578, 0.2711, 0.2578],\n",
       "           [0.8222, 0.9467, 0.9378,  ..., 0.2578, 0.2667, 0.2578],\n",
       "           ...,\n",
       "           [0.5422, 0.5333, 0.5244,  ..., 0.6622, 0.6622, 0.6622],\n",
       "           [0.5378, 0.5289, 0.5200,  ..., 0.6622, 0.6622, 0.6622],\n",
       "           [0.5378, 0.5289, 0.5200,  ..., 0.6622, 0.6622, 0.6622]],\n",
       "\n",
       "          [[0.7467, 0.8667, 0.8667,  ..., 0.2489, 0.2489, 0.2400],\n",
       "           [0.7422, 0.8667, 0.8667,  ..., 0.2444, 0.2489, 0.2356],\n",
       "           [0.7422, 0.8667, 0.8622,  ..., 0.2444, 0.2444, 0.2356],\n",
       "           ...,\n",
       "           [0.5111, 0.5067, 0.4978,  ..., 0.6444, 0.6444, 0.6444],\n",
       "           [0.5067, 0.5022, 0.4933,  ..., 0.6444, 0.6444, 0.6444],\n",
       "           [0.5067, 0.5022, 0.4933,  ..., 0.6444, 0.6444, 0.6444]],\n",
       "\n",
       "          [[0.6978, 0.8178, 0.8089,  ..., 0.2578, 0.2578, 0.2489],\n",
       "           [0.6933, 0.8178, 0.8089,  ..., 0.2533, 0.2578, 0.2444],\n",
       "           [0.6933, 0.8178, 0.8044,  ..., 0.2533, 0.2533, 0.2444],\n",
       "           ...,\n",
       "           [0.5689, 0.5556, 0.5467,  ..., 0.6667, 0.6667, 0.6667],\n",
       "           [0.5644, 0.5511, 0.5422,  ..., 0.6667, 0.6667, 0.6667],\n",
       "           [0.5644, 0.5511, 0.5422,  ..., 0.6667, 0.6667, 0.6667]]],\n",
       "\n",
       "\n",
       "         [[[0.8311, 0.9511, 0.9467,  ..., 0.2578, 0.2533, 0.2667],\n",
       "           [0.8267, 0.9511, 0.9467,  ..., 0.2489, 0.2444, 0.2578],\n",
       "           [0.8267, 0.9511, 0.9422,  ..., 0.2444, 0.2400, 0.2444],\n",
       "           ...,\n",
       "           [0.5600, 0.5378, 0.5289,  ..., 0.6622, 0.6578, 0.6533],\n",
       "           [0.5644, 0.5422, 0.5289,  ..., 0.6711, 0.6622, 0.6533],\n",
       "           [0.5644, 0.5378, 0.5289,  ..., 0.6800, 0.6667, 0.6533]],\n",
       "\n",
       "          [[0.7511, 0.8711, 0.8711,  ..., 0.2533, 0.2489, 0.2622],\n",
       "           [0.7467, 0.8711, 0.8711,  ..., 0.2444, 0.2400, 0.2533],\n",
       "           [0.7467, 0.8711, 0.8667,  ..., 0.2400, 0.2356, 0.2400],\n",
       "           ...,\n",
       "           [0.5200, 0.5022, 0.4933,  ..., 0.6444, 0.6400, 0.6356],\n",
       "           [0.5200, 0.4978, 0.4844,  ..., 0.6533, 0.6444, 0.6356],\n",
       "           [0.5200, 0.4933, 0.4844,  ..., 0.6622, 0.6489, 0.6356]],\n",
       "\n",
       "          [[0.7022, 0.8222, 0.8133,  ..., 0.2622, 0.2578, 0.2711],\n",
       "           [0.6978, 0.8222, 0.8133,  ..., 0.2533, 0.2489, 0.2622],\n",
       "           [0.6978, 0.8222, 0.8089,  ..., 0.2489, 0.2444, 0.2489],\n",
       "           ...,\n",
       "           [0.5778, 0.5511, 0.5422,  ..., 0.6667, 0.6622, 0.6578],\n",
       "           [0.5778, 0.5511, 0.5378,  ..., 0.6756, 0.6667, 0.6578],\n",
       "           [0.5778, 0.5467, 0.5378,  ..., 0.6844, 0.6711, 0.6578]]],\n",
       "\n",
       "\n",
       "         [[[0.8311, 0.9556, 0.9511,  ..., 0.2444, 0.2444, 0.2444],\n",
       "           [0.8311, 0.9556, 0.9511,  ..., 0.2400, 0.2400, 0.2489],\n",
       "           [0.8267, 0.9556, 0.9511,  ..., 0.2311, 0.2356, 0.2489],\n",
       "           ...,\n",
       "           [0.4889, 0.2622, 0.2222,  ..., 0.6578, 0.6578, 0.6578],\n",
       "           [0.4800, 0.2089, 0.1644,  ..., 0.6533, 0.6578, 0.6578],\n",
       "           [0.4978, 0.2489, 0.2044,  ..., 0.6533, 0.6533, 0.6578]],\n",
       "\n",
       "          [[0.7600, 0.8844, 0.8800,  ..., 0.2533, 0.2533, 0.2533],\n",
       "           [0.7600, 0.8844, 0.8800,  ..., 0.2489, 0.2489, 0.2578],\n",
       "           [0.7556, 0.8844, 0.8800,  ..., 0.2400, 0.2444, 0.2578],\n",
       "           ...,\n",
       "           [0.4444, 0.2222, 0.1778,  ..., 0.6400, 0.6400, 0.6400],\n",
       "           [0.4222, 0.1644, 0.1111,  ..., 0.6356, 0.6400, 0.6400],\n",
       "           [0.4400, 0.1911, 0.1511,  ..., 0.6356, 0.6356, 0.6400]],\n",
       "\n",
       "          [[0.6889, 0.8133, 0.8089,  ..., 0.2578, 0.2578, 0.2578],\n",
       "           [0.6889, 0.8133, 0.8089,  ..., 0.2533, 0.2533, 0.2622],\n",
       "           [0.6844, 0.8133, 0.8089,  ..., 0.2444, 0.2489, 0.2622],\n",
       "           ...,\n",
       "           [0.5022, 0.2800, 0.2311,  ..., 0.6622, 0.6622, 0.6622],\n",
       "           [0.4844, 0.2222, 0.1644,  ..., 0.6578, 0.6622, 0.6622],\n",
       "           [0.5022, 0.2533, 0.2044,  ..., 0.6578, 0.6578, 0.6622]]],\n",
       "\n",
       "\n",
       "         [[[0.8533, 0.9733, 0.9689,  ..., 0.2756, 0.2489, 0.2311],\n",
       "           [0.8489, 0.9733, 0.9689,  ..., 0.2756, 0.2622, 0.2533],\n",
       "           [0.8489, 0.9733, 0.9644,  ..., 0.2622, 0.2622, 0.2622],\n",
       "           ...,\n",
       "           [0.4133, 0.2667, 0.2400,  ..., 0.6533, 0.6533, 0.6533],\n",
       "           [0.5200, 0.2978, 0.2489,  ..., 0.6533, 0.6533, 0.6533],\n",
       "           [0.6578, 0.3289, 0.2089,  ..., 0.6533, 0.6533, 0.6533]],\n",
       "\n",
       "          [[0.7600, 0.8800, 0.8756,  ..., 0.2711, 0.2444, 0.2267],\n",
       "           [0.7556, 0.8800, 0.8756,  ..., 0.2711, 0.2578, 0.2489],\n",
       "           [0.7556, 0.8800, 0.8711,  ..., 0.2578, 0.2578, 0.2578],\n",
       "           ...,\n",
       "           [0.3956, 0.2489, 0.2222,  ..., 0.6489, 0.6489, 0.6489],\n",
       "           [0.5022, 0.2800, 0.2311,  ..., 0.6489, 0.6489, 0.6489],\n",
       "           [0.6400, 0.3111, 0.1911,  ..., 0.6489, 0.6489, 0.6489]],\n",
       "\n",
       "          [[0.6933, 0.8133, 0.8089,  ..., 0.2800, 0.2533, 0.2356],\n",
       "           [0.6889, 0.8133, 0.8089,  ..., 0.2800, 0.2667, 0.2578],\n",
       "           [0.6889, 0.8133, 0.8044,  ..., 0.2667, 0.2667, 0.2667],\n",
       "           ...,\n",
       "           [0.4222, 0.2756, 0.2444,  ..., 0.6667, 0.6667, 0.6667],\n",
       "           [0.5289, 0.3067, 0.2533,  ..., 0.6667, 0.6667, 0.6667],\n",
       "           [0.6667, 0.3378, 0.2133,  ..., 0.6667, 0.6667, 0.6667]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[0.3644, 0.4400, 0.3911,  ..., 0.2133, 0.2400, 0.2756],\n",
       "           [0.3822, 0.4444, 0.3867,  ..., 0.2267, 0.2489, 0.2667],\n",
       "           [0.3689, 0.4311, 0.3867,  ..., 0.2356, 0.2489, 0.2489],\n",
       "           ...,\n",
       "           [0.5778, 0.5867, 0.5733,  ..., 0.3067, 0.3067, 0.3733],\n",
       "           [0.6044, 0.5956, 0.5644,  ..., 0.3111, 0.3156, 0.3867],\n",
       "           [0.5644, 0.5689, 0.5778,  ..., 0.3156, 0.3200, 0.3956]],\n",
       "\n",
       "          [[0.3689, 0.4444, 0.3956,  ..., 0.2222, 0.2489, 0.2844],\n",
       "           [0.3867, 0.4489, 0.3911,  ..., 0.2356, 0.2578, 0.2756],\n",
       "           [0.3733, 0.4356, 0.3911,  ..., 0.2444, 0.2578, 0.2578],\n",
       "           ...,\n",
       "           [0.5600, 0.5689, 0.5556,  ..., 0.2756, 0.2711, 0.3378],\n",
       "           [0.5867, 0.5778, 0.5467,  ..., 0.2800, 0.2800, 0.3511],\n",
       "           [0.5467, 0.5511, 0.5600,  ..., 0.2844, 0.2844, 0.3600]],\n",
       "\n",
       "          [[0.3867, 0.4622, 0.4133,  ..., 0.2267, 0.2533, 0.2889],\n",
       "           [0.4044, 0.4667, 0.4089,  ..., 0.2400, 0.2622, 0.2800],\n",
       "           [0.3911, 0.4533, 0.4089,  ..., 0.2489, 0.2622, 0.2622],\n",
       "           ...,\n",
       "           [0.6133, 0.6222, 0.6089,  ..., 0.3067, 0.3200, 0.3867],\n",
       "           [0.6400, 0.6311, 0.6000,  ..., 0.3111, 0.3289, 0.4000],\n",
       "           [0.6000, 0.6044, 0.6133,  ..., 0.3156, 0.3333, 0.4089]]],\n",
       "\n",
       "\n",
       "         [[[0.2889, 0.2711, 0.2889,  ..., 0.3200, 0.3244, 0.3022],\n",
       "           [0.2978, 0.2933, 0.3067,  ..., 0.2889, 0.2978, 0.2844],\n",
       "           [0.3022, 0.3111, 0.3289,  ..., 0.2711, 0.2844, 0.2844],\n",
       "           ...,\n",
       "           [1.0267, 1.0400, 1.0978,  ..., 0.3689, 0.3689, 0.4222],\n",
       "           [1.0533, 1.0222, 0.9644,  ..., 0.4089, 0.4000, 0.4489],\n",
       "           [1.0578, 0.9689, 0.7289,  ..., 0.4533, 0.4400, 0.4889]],\n",
       "\n",
       "          [[0.2933, 0.2756, 0.2933,  ..., 0.3556, 0.3600, 0.3378],\n",
       "           [0.3022, 0.2978, 0.3111,  ..., 0.3244, 0.3333, 0.3200],\n",
       "           [0.3067, 0.3156, 0.3333,  ..., 0.3067, 0.3200, 0.3200],\n",
       "           ...,\n",
       "           [1.0356, 1.0489, 1.1067,  ..., 0.3511, 0.3422, 0.3956],\n",
       "           [1.0711, 1.0400, 0.9733,  ..., 0.3911, 0.3733, 0.4222],\n",
       "           [1.0756, 0.9867, 0.7378,  ..., 0.4356, 0.4133, 0.4622]],\n",
       "\n",
       "          [[0.3111, 0.2933, 0.3111,  ..., 0.3556, 0.3600, 0.3378],\n",
       "           [0.3200, 0.3156, 0.3289,  ..., 0.3244, 0.3333, 0.3200],\n",
       "           [0.3244, 0.3333, 0.3511,  ..., 0.3067, 0.3200, 0.3200],\n",
       "           ...,\n",
       "           [1.0844, 1.0978, 1.1333,  ..., 0.3778, 0.3911, 0.4444],\n",
       "           [1.1200, 1.0889, 1.0222,  ..., 0.4178, 0.4222, 0.4711],\n",
       "           [1.1244, 1.0356, 0.7867,  ..., 0.4622, 0.4622, 0.5111]]],\n",
       "\n",
       "\n",
       "         [[[0.3067, 0.2844, 0.2800,  ..., 0.2000, 0.1911, 0.2178],\n",
       "           [0.3111, 0.2844, 0.2800,  ..., 0.1911, 0.1867, 0.2178],\n",
       "           [0.3111, 0.2889, 0.2756,  ..., 0.2089, 0.2089, 0.2356],\n",
       "           ...,\n",
       "           [0.5911, 0.5200, 0.4889,  ..., 0.3778, 0.3911, 0.4311],\n",
       "           [0.5511, 0.4667, 0.5022,  ..., 0.3822, 0.3733, 0.3956],\n",
       "           [0.4667, 0.5289, 0.5600,  ..., 0.4400, 0.4311, 0.4444]],\n",
       "\n",
       "          [[0.3111, 0.2889, 0.2844,  ..., 0.2533, 0.2444, 0.2711],\n",
       "           [0.3156, 0.2889, 0.2844,  ..., 0.2444, 0.2400, 0.2711],\n",
       "           [0.3156, 0.2933, 0.2800,  ..., 0.2622, 0.2622, 0.2889],\n",
       "           ...,\n",
       "           [0.5733, 0.5022, 0.4711,  ..., 0.3600, 0.3733, 0.4133],\n",
       "           [0.5333, 0.4489, 0.4844,  ..., 0.3644, 0.3556, 0.3778],\n",
       "           [0.4489, 0.5111, 0.5422,  ..., 0.4222, 0.4133, 0.4267]],\n",
       "\n",
       "          [[0.3289, 0.3067, 0.3022,  ..., 0.2533, 0.2444, 0.2711],\n",
       "           [0.3333, 0.3067, 0.3022,  ..., 0.2444, 0.2400, 0.2711],\n",
       "           [0.3333, 0.3111, 0.2978,  ..., 0.2622, 0.2622, 0.2889],\n",
       "           ...,\n",
       "           [0.6222, 0.5511, 0.5200,  ..., 0.3822, 0.4000, 0.4400],\n",
       "           [0.5822, 0.4978, 0.5333,  ..., 0.3867, 0.3822, 0.4044],\n",
       "           [0.4978, 0.5600, 0.5911,  ..., 0.4444, 0.4400, 0.4533]]],\n",
       "\n",
       "\n",
       "         [[[0.2756, 0.3022, 0.3200,  ..., 0.2444, 0.2133, 0.2133],\n",
       "           [0.2667, 0.2844, 0.2978,  ..., 0.2889, 0.2578, 0.2578],\n",
       "           [0.2622, 0.2667, 0.2711,  ..., 0.2800, 0.2578, 0.2578],\n",
       "           ...,\n",
       "           [0.5467, 0.5467, 0.5511,  ..., 0.4756, 0.4578, 0.4756],\n",
       "           [0.5556, 0.5600, 0.5644,  ..., 0.4933, 0.4800, 0.4844],\n",
       "           [0.5822, 0.5822, 0.5822,  ..., 0.4756, 0.4622, 0.4578]],\n",
       "\n",
       "          [[0.2889, 0.3156, 0.3333,  ..., 0.2622, 0.2311, 0.2311],\n",
       "           [0.2800, 0.2978, 0.3111,  ..., 0.3067, 0.2756, 0.2756],\n",
       "           [0.2756, 0.2800, 0.2844,  ..., 0.2978, 0.2756, 0.2756],\n",
       "           ...,\n",
       "           [0.5289, 0.5289, 0.5333,  ..., 0.4667, 0.4400, 0.4578],\n",
       "           [0.5378, 0.5422, 0.5467,  ..., 0.4844, 0.4622, 0.4667],\n",
       "           [0.5644, 0.5644, 0.5644,  ..., 0.4667, 0.4444, 0.4400]],\n",
       "\n",
       "          [[0.3067, 0.3333, 0.3511,  ..., 0.2667, 0.2356, 0.2356],\n",
       "           [0.2978, 0.3156, 0.3289,  ..., 0.3111, 0.2800, 0.2800],\n",
       "           [0.2933, 0.2978, 0.3022,  ..., 0.3022, 0.2800, 0.2800],\n",
       "           ...,\n",
       "           [0.5778, 0.5778, 0.5822,  ..., 0.4933, 0.4889, 0.5067],\n",
       "           [0.5867, 0.5911, 0.5956,  ..., 0.5111, 0.5111, 0.5156],\n",
       "           [0.6133, 0.6133, 0.6133,  ..., 0.4933, 0.4933, 0.4889]]]]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
